{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (1.62.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (1.62.1)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (2.8.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (2.6.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from qdrant-client) (2.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (0.0.31)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (0.1.40)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (0.1.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain_openai) (1.16.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.2.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from pydantic>=1.10.8->qdrant-client) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain_openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting unstructured\n",
      "  Downloading unstructured-0.13.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-5.2.1-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tabulate (from unstructured)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured) (2.31.0)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured) (0.6.4)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2024.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 71.7/981.5 kB 2.0 MB/s eta 0:00:01\n",
      "     ------- ------------------------------ 204.8/981.5 kB 2.5 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 358.4/981.5 kB 2.8 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 512.0/981.5 kB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 634.9/981.5 kB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 788.5/981.5 kB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 870.4/981.5 kB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 2.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.7.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured) (4.10.0)\n",
      "Collecting unstructured-client<=0.18.0 (from unstructured)\n",
      "  Downloading unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (3.3.2)\n",
      "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client<=0.18.0->unstructured)\n",
      "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: idna>=3.4 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (3.6)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client<=0.18.0->unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: marshmallow>=3.19.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (3.21.1)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (1.0.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (2.9.0)\n",
      "Requirement already satisfied: six>=1.16.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (1.16.0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (0.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from unstructured-client<=0.18.0->unstructured) (2.2.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->unstructured)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from nltk->unstructured) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from nltk->unstructured) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jcasas\\appdata\\local\\anaconda3\\envs\\idea\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Downloading unstructured-0.13.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.9 MB 2.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 3.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 147.9/147.9 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 143.4/199.4 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.4/199.4 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 143.4/433.8 kB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading lxml-5.2.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.8 MB 8.3 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.3/3.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/3.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.6/3.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.7/3.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.8/3.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.1/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.2/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.5/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.9/3.8 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.9/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.2/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.3/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.5/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.6/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.8/3.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.8/3.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.9/3.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.1/3.8 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.3/3.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.5/3.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 2.4 MB/s eta 0:00:00\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.7 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 194.6/274.7 kB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 266.2/274.7 kB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 274.7/274.7 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading rapidfuzz-3.7.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/1.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/1.6 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.6 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.9/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.1/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.6/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.6 MB/s eta 0:00:00\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=95f6c8d6bc9adffd3118fcf05c8f9ff711144cd2a221bab96b47b027685879a3\n",
      "  Stored in directory: c:\\users\\jcasas\\appdata\\local\\pip\\cache\\wheels\\95\\03\\7d\\59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, wrapt, tabulate, soupsieve, rapidfuzz, python-magic, python-iso639, lxml, langdetect, jsonpath-python, joblib, emoji, click, chardet, backoff, nltk, dataclasses-json-speakeasy, beautifulsoup4, unstructured-client, unstructured\n",
      "Successfully installed backoff-2.2.1 beautifulsoup4-4.12.3 chardet-5.2.0 click-8.1.7 dataclasses-json-speakeasy-0.5.11 emoji-2.11.0 filetype-1.2.0 joblib-1.3.2 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.2.1 nltk-3.8.1 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.7.0 soupsieve-2.5 tabulate-0.9.0 unstructured-0.13.2 unstructured-client-0.18.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client langchain langchain_openai docx2txt\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Document(page_content='I Love Dogs and cats\\n\\n\\n\\n1\\n\\n1', metadata={'source': '../mi_proyecto/uploaded_files/Dogs.docx'})]\n",
      "[Document(page_content='I love pizza\\n\\n\\n\\n1\\n\\n1', metadata={'source': '../mi_proyecto/uploaded_files/Pizza.docx'})]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Ubicación de los archivos\n",
    "folder_path = \"../mi_proyecto/uploaded_files/\"\n",
    "\n",
    "# Obtener una lista de todos los archivos en la ubicación especificada\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Inicializar la lista de documentos\n",
    "documents = []\n",
    "docs = []\n",
    "\n",
    "# Leer el contenido de cada archivo de Word y agregarlo a la lista de documentos\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.append(loader.load())\n",
    "\n",
    "print(len(documents))  # Imprimir la cantidad de documentos cargados\n",
    "\n",
    "# Ahora puedes continuar con tu proceso de división de texto y embedding\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "for doc in documents:\n",
    "    print(doc)\n",
    "    docs.append(text_splitter.split_documents(doc))\n",
    "\n",
    "# Obtener embeddings utilizando OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Continuar con tu proceso de embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "loader = Docx2txtLoader(\"../mi_proyecto/uploaded_files/fake.docx\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "\n",
    "\n",
    "# folder_path=\"../mi_proyecto/uploaded_files/\"\n",
    "# txt_loader = DirectoryLoader(folder_path, glob=\"*.docx\", loader_cls=UnstructuredWordDocumentLoader)\n",
    "# documents = txt_loader.load()\n",
    "# print(len(documents))\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# docs = text_splitter.split_documents(documents)\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='VICEPRESIDENCIA DIGITAL\\n\\n\\n\\nBogotá D.C., 20 de Noviembre de 2023\\n\\n\\n\\nINFORME TÉCNICO DE ACTIVIDADES\\n\\nNÚMERO DEL CONTRATO\\n\\nORDEN DE SERVICIO No. 9360765 BAJO CONTRATO MARCO No.3048537\\n\\nCONTRATISTA:\\n\\nASESOFTWARE SAS\\n\\nNIT:\\n\\n800135532-9\\n\\nOBJETO:\\n\\nDesarrollar actividades de aseguramiento y gestión de iniciativas de manera transversal a las iniciativas de la fábrica BI\\n\\nLUGAR DE EJECUCIÓN\\n\\nBOGOTÁ\\n\\n\\t\\t\\tPERIODO\\tDE\\tEJECUCIÓN\\tDE ACTIVIDADES\\n\\n20/10/2023 al 20/11/2023\\n\\n\\t\\t\\tVALOR\\tDE\\tLAS\\tACTIVIDADES EJECUTADAS\\n\\nDieciséis Millones ciento ochenta y dos Mil Cuatrocientos Noventa y dos pesos MCTE (COP\\n\\n$16.182.492,00) SIN INCLUIR EL VALOR DEL IMPUESTO AL VALOR AGREGADO (IVA).\\n\\nLÍDER(ES) PROYECTO\\n\\nGeraldyne Chavez Mur\\n\\nACTIVIDADES REALIZADAS\\n\\n\\n\\n\\n\\nEl CONTRATISTA, en el periodo comprendido entre el 20/10/2023 al 20/11/2023, prestó los    servicios asegurando el desarrollo, implementación, gestión y operación de soluciones de software automatizadas para Ecopetrol y su grupo empresarial, ejecutado e implementado sobre marcos ágiles de trabajo.\\n\\n\\n\\nRevisar el avance de los componentes del proyecto y el seguimiento desde el Marco No. 3048537. Los servicios prestados en el periodo descrito por ítem del Contrato son:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEntregable\\n\\nEvidencia\\n\\nGGO Comportamiento Financiero.\\n\\nVista reporte general de los contratos – Power BI (tablero)\\n\\nSe realiza actualización del tema y paleta de colores usado en la vista considerando la actualización realizada por UX.\\n\\nSe realizan ajuste de tarjeta y gráficos de\\n\\nVista tablero contractual general de los contratos – Power BI. \\n\\nSe ajustan tarjetas y filtros asociados a la vista.\\n\\nSe realizan las conexiones y cargas de nuevas listas y vistas de las fuentes en denodo.\\n\\nSe actualiza el modelo de datos considerando las nuevas listas y vistas de la vista.\\n\\nSe realiza redireccionamiento de los campos a las nuevas fuentes de datos cargadas de Denodo\\n\\nSe realiza actualización del tema y paleta de colores usado en la vista considerando la actualización realizada por UX.\\n\\nSe realizan ajustes funcionales a la vista de acuerdo a las sesiones de UAT con los administradores del contrato\\n\\nVista seguimiento financiero de los contratos – Power BI (tablero).\\n\\nRefinamiento de la HU (402844) con las observaciones y consideraciones del usuario.\\n\\nConstrucción de base de datos a ser usada como fuente en el mockup de la “vista del seguimiento presupuestal y financiero en Power BI” y creación de algunas variables requeridas para la conformación de la vista.\\n\\nConstrucción del mockup de la “vista del seguimiento presupuestal y financiero en Power BI” incluyendo los filtros, gráficos y reporte detallado sugerido por los usuarios del reporte.\\n\\nVista seguimiento especifico de contratos – Power BI.\\n\\nRefinamiento de la HU (402854) con las observaciones y consideraciones del usuario.\\n\\nConstrucción de base de datos a ser usada como fuente en el mockup de la “vista contractual especifica del contrato en Power BI” \\n\\nSe realiza actualización del tema y paleta de colores usado en la vista considerando la actualización realizada por UX.\\n\\nSe realizan ajustes de funcionalidad de la vista: Filtros, marcadores y componentes visuales.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTablero - Reporte general de contratos\\n\\nVista seguimiento del seguimiento volumétrico y financiero de otros contratos en Power BI\\n\\nRefinamiento de la HU (402858) con las observaciones y consideraciones del usuario.\\n\\nSe realiza actualización del tema y paleta de colores usado en la vista considerando la actualización realizada por UX.\\n\\nSe realizan ajustes de funcionalidad de la vista: Filtros, marcadores y componentes visuales.\\n\\n\\n\\n\\n\\n\\n\\nActualización del manual técnico de Power BI del tablero comportamiento financiero con las 4 vistas adicionales trabajadas.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nManual Tecnico Power BI - Comportamiento financiero GGO\\n\\n\\n\\n\\n\\nTeniendo en cuenta todo lo mencionado anteriormente los valores a cancelar al CONTRATISTA por los trabajos ejecutados y recibidos a satisfacción conforme lo estipulado en los documentos del contrato por parte de Ecopetrol S.A. son:\\n\\n\\n\\n\\n\\n\\n\\nPerfil\\n\\nUnidad de Medida\\n\\nPrecio Unitario en COP\\n\\n\\n\\nCantidad\\n\\nValor Para Facturar\\n\\nPequeña desarrollo e implementación 1 célula pequeña (analista de datos) \\n\\n\\n\\nSemanas\\n\\n\\n\\n$4.045.623\\n\\n\\n\\n4\\n\\n\\n\\n$16.182.492,00 \\n\\n\\n\\nTotal, Sin IVA\\n\\n\\n\\n$16.182.492,00 \\n\\n\\n\\nIVA\\n\\n$3.074.673,48\\n\\n\\n\\nTotal, con IVA\\n\\n\\n\\n$19.257.165,48 \\n\\n\\n\\nEl servicio por locación, para el valor total ejecutado en el periodo se discrimina de la siguiente manera:\\n\\n\\n\\n\\n\\n\\n\\nLocalidad de Ejecución\\n\\nValor Ejecución COP $ sin IVA\\n\\n% de Ejecución\\n\\n\\n\\nBogotá\\n\\n\\n\\n$16.182.492,00\\n\\n\\n\\n100%\\n\\n\\n\\nTotal\\n\\n\\n\\n$16.182.492,00\\n\\n\\n\\n100%\\n\\n\\n\\nNO CONFORMIDADES TECNICAS\\n\\n\\n\\nNo aplica\\n\\n\\n\\nFALLAS DE CONTROL/INCIDENTES\\n\\n\\n\\nNo aplica. Cordialmente,\\n\\n\\n\\n\\n\\n\\n\\nGeraldyne Chavez Mur\\n\\nLíder de fábrica ECOPETROL S.A.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJohn Jorge Clavijo Jiménez\\n\\nDirector de Proyecto\\n\\nASESOFTWARE SAS\\n\\n\\n\\n1\\n\\n1', metadata={'source': '../mi_proyecto/uploaded_files/fake.docx'})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000016B152614E0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000016B152604C0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qdrant \u001b[38;5;241m=\u001b[39m \u001b[43mQdrant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:memory:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Local mode with in-memory storage only\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\langchain_core\\vectorstores.py:548\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\langchain_core\\vectorstores.py:548\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALLAS DE CONTROL/INCIDENTES\n",
      "\n",
      "No aplica. Cordialmente,\n",
      "\n",
      "Geraldyne Chavez Mur\n",
      "\n",
      "Líder de fábrica ECOPETROL S.A.\n",
      "\n",
      "John Jorge Clavijo Jiménez\n",
      "\n",
      "Director de Proyecto\n",
      "\n",
      "ASESOFTWARE SAS\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "Score: 0.9779895948092672\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"FALLAS DE CONTROL/INCIDENTES\n",
    "\n",
    "No aplica. Cordialmente,\n",
    "\n",
    "Geraldyne Chavez Mur\n",
    "\n",
    "Líder de fábrica ECOPETROL S.A.\n",
    "\n",
    "John Jorge Clavijo Jiménez\n",
    "\n",
    "Director de Proyecto\"\"\"\n",
    "found_docs = qdrant.similarity_search_with_score(query)\n",
    "\n",
    "document, score = found_docs[0]\n",
    "print(document.page_content)\n",
    "print(f\"\\nScore: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "loader = Docx2txtLoader(\"../mi_proyecto/uploaded_files/fake.docx\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcasas\\AppData\\Local\\Temp\\ipykernel_17136\\4133830897.py:80: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n",
      "  client.upload_records(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('status', <CollectionStatus.GREEN: 'green'>),\n",
       " ('optimizer_status', <OptimizersStatusOneOf.OK: 'ok'>),\n",
       " ('vectors_count', 2),\n",
       " ('indexed_vectors_count', 0),\n",
       " ('points_count', 2),\n",
       " ('segments_count', 1),\n",
       " ('config',\n",
       "  CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1536, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None)),\n",
       " ('payload_schema', {})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import models, QdrantClient\n",
    "\n",
    "\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "\n",
    "\n",
    "import openai\n",
    "import qdrant_client\n",
    "\n",
    "\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import CollectionStatus\n",
    "\n",
    "\n",
    "#from langchain_openai import Distance, VectorParams\n",
    "\n",
    "def create_vector_db():\n",
    "    \n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    client = qdrant_client.QdrantClient(\":memory:\")\n",
    "\n",
    "    \n",
    "    collection_name = \"test\"\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=1536,\n",
    "            distance=Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "\n",
    "    return client\n",
    "        \n",
    "\n",
    "client = create_vector_db()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from qdrant_client.models import PointStruct\n",
    "import openai\n",
    "import qdrant_client\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "openai_client = openai.Client(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"Qdrant is the best vector search engine!\",\n",
    "    \"Loved by Enterprises and everyone building for low latency, high performance, and scale.\",\n",
    "]\n",
    "\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "result = openai_client.embeddings.create(input=texts, model=embedding_model)\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=idx,\n",
    "        vector=data.embedding,\n",
    "        payload={\"text\": text},\n",
    "    )\n",
    "    for idx, (data, text) in enumerate(zip(result.data, texts))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "client.upload_records(\n",
    "        collection_name=\"test\",\n",
    "        records=points\n",
    "    )  \n",
    "\n",
    "\n",
    "collection_info = client.get_collection(collection_name=\"test\")\n",
    "list(collection_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcasas\\AppData\\Local\\Temp\\ipykernel_17136\\3705897700.py:19: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n",
      "  client.upload_records(\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"I Love Pizza\",\n",
    "    \"I Love burgers\",\n",
    "    \"I Love my dog\", \"HEllo\"\n",
    "]\n",
    "\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=idx,\n",
    "        vector=data.embedding,\n",
    "        payload={\"text\": text},\n",
    "    )\n",
    "    for idx, (data, text) in enumerate(zip(result.data, texts))\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "client.upload_records(\n",
    "        collection_name=\"test\",\n",
    "        records=points\n",
    "    )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QdrantClient' object has no attribute 'similarity_search_with_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mI Love pizza\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m found_docs \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m(query)\n\u001b[0;32m      4\u001b[0m document, score \u001b[38;5;241m=\u001b[39m found_docs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(document\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QdrantClient' object has no attribute 'similarity_search_with_score'"
     ]
    }
   ],
   "source": [
    "query = \"\"\"I Love pizza\"\"\"\n",
    "found_docs = client.similarity_search_with_score(query)\n",
    "\n",
    "document, score = found_docs[0]\n",
    "print(document.page_content)\n",
    "print(f\"\\nScore: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m     17\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m---> 19\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m qdrant\u001b[38;5;241m.\u001b[39mupload_records(\n\u001b[0;32m     22\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_books\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     records\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     ]\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     35\u001b[0m collection_info \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_collection(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:546\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:517\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:310\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m001\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;66;03m# See: https://github.com/openai/openai-python/\u001b[39;00m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m#      issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 310\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length):\n",
      "File \u001b[1;32mc:\\Users\\jcasas\\AppData\\Local\\anaconda3\\envs\\idea\\lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "loader = Docx2txtLoader(\"../mi_proyecto/uploaded_files/fake.docx\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embedding = embeddings.embed_query(documents)\n",
    "\n",
    "qdrant.upload_records(\n",
    "    collection_name=\"my_books\",\n",
    "    records=[\n",
    "        models.Record(\n",
    "            id=idx,\n",
    "            vector=embedding,\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(documents)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "collection_info = client.get_collection(collection_name=\"test\")\n",
    "list(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "import openai\n",
    "import qdrant_client\n",
    "from qdrant_client.models import PointStruct\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "\n",
    "loader = DirectoryLoader('../mi_proyecto/uploaded_files', glob=\"**/*.docx\", show_progress=True)\n",
    "in_docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(in_docs)\n",
    "\n",
    "client = qdrant_client.QdrantClient(\":memory:\")\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "list_docs = [doc.page_content for doc in texts]\n",
    "\n",
    "result = openai_client.embeddings.create(input=list_docs, model=embedding_model)\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=idx,\n",
    "        vector=data.embedding,\n",
    "        payload={\"text\": text},\n",
    "    )\n",
    "    for idx, (data, text) in enumerate(zip(result.data, list_docs))\n",
    "]\n",
    "\n",
    "\n",
    "collection_name = \"example_collection\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=1536,\n",
    "        distance=Distance.COSINE,\n",
    "    ),\n",
    ")\n",
    "client.upsert(collection_name, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_api_key_from_config(config_file_path):\n",
    "    try:\n",
    "        with open(config_file_path, 'r') as config_file:\n",
    "            config = json.load(config_file)\n",
    "            print(config.get(\"openai_api_key\"))\n",
    "            return config.get(\"openai_api_key\")\n",
    "    except FileNotFoundError:\n",
    "        # Manejar el caso en el que el archivo de configuración no está presente\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Manejar el caso en el que el archivo de configuración no es válido JSON\n",
    "        return None\n",
    "\n",
    "config_file_path = \"config.json\"\n",
    "openai_key = load_api_key_from_config(config_file_path)\n",
    "print(openai_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
